{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a4c2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from physics import combined_loss, Emb, ANN, Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e261d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4910421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(df):\n",
    "    t = torch.tensor(df['t_normalized'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    t.requires_grad_(True)  \n",
    "\n",
    "    T_C_norm = torch.tensor(df['T_celsius_norm'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    T_C = torch.tensor(df['T_celsius'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    I = torch.tensor(df['I_amperes_norm'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    bat_idx = torch.tensor(df['battery_idx'].values, dtype=torch.long)\n",
    "    C_target = torch.tensor(df['C_target'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    return t, T_C, T_C_norm, I, bat_idx, C_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb8d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "lambda_phys = 1.0  \n",
    "lambda_data = 1.0\n",
    "warmup_epochs = 200 \n",
    "\n",
    "all_params = list(Emb.parameters()) + list(ANN.parameters()) + list(Param.parameters())\n",
    "optimizer = optim.Adam(all_params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd7c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total Loss = 5.550944, Physics Loss = 0.999983, Data Loss = 2.775472, RHS = -0.000000, dCdt = 0.117500, Arrhenius Term = 0.000000, I_mag = 0.340005\n",
      "Epoch 100: Total Loss = 0.142567, Physics Loss = 0.999994, Data Loss = 0.071283, RHS = -0.000000, dCdt = -0.351211, Arrhenius Term = 0.000000, I_mag = 0.340005\n",
      "Switching to L-BFGS optimizer...\n",
      "Epoch 200: Total Loss = 0.002584, Physics Loss = 0.999996, Data Loss = 0.001292, RHS = -0.000000, dCdt = -0.488614, Arrhenius Term = 0.000000, I_mag = 0.340005\n",
      "Epoch 300: Total Loss = 0.000335, Physics Loss = 0.999971, Data Loss = 0.000167, RHS = -0.000000, dCdt = -0.312709, Arrhenius Term = 0.000000, I_mag = 0.340005\n",
      "Epoch 400: Total Loss = 0.000335, Physics Loss = 0.999973, Data Loss = 0.000167, RHS = -0.000000, dCdt = -0.313182, Arrhenius Term = 0.000000, I_mag = 0.340005\n",
      "Epoch 500: Total Loss = 0.000335, Physics Loss = 0.999974, Data Loss = 0.000167, RHS = -0.000000, dCdt = -0.313424, Arrhenius Term = 0.000000, I_mag = 0.340005\n",
      "Epoch 600: Total Loss = 0.000335, Physics Loss = 0.999975, Data Loss = 0.000167, RHS = -0.000000, dCdt = -0.313611, Arrhenius Term = 0.000000, I_mag = 0.340005\n",
      "Epoch 700: Total Loss = 0.000335, Physics Loss = 0.999976, Data Loss = 0.000167, RHS = -0.000000, dCdt = -0.313767, Arrhenius Term = 0.000000, I_mag = 0.340005\n",
      "Epoch 800: Total Loss = 0.000335, Physics Loss = 0.999976, Data Loss = 0.000167, RHS = -0.000000, dCdt = -0.313894, Arrhenius Term = 0.000000, I_mag = 0.340005\n",
      "Epoch 900: Total Loss = 0.000335, Physics Loss = 0.999976, Data Loss = 0.000167, RHS = -0.000000, dCdt = -0.313920, Arrhenius Term = 0.000000, I_mag = 0.340005\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    if epoch == warmup_epochs:\n",
    "        print(\"Switching to L-BFGS optimizer...\")\n",
    "        optimizer = optim.LBFGS(all_params, lr=0.1, max_iter=20, history_size=10)\n",
    "    \n",
    "    # Prepare batch\n",
    "    t, T_C, T_C_norm, I, bat_idx, C_target = prepare_batch(train_df)\n",
    "    \n",
    "    if epoch < warmup_epochs:\n",
    "        # Adam training\n",
    "        optimizer.zero_grad()\n",
    "        total_loss, params, physics_loss, data_loss, rhs, dCdt, arrhenius_term, I_mag = combined_loss(\n",
    "            t, T_C, T_C_norm, I, bat_idx, C_target, lambda_phys, lambda_data, epoch\n",
    "        )\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        # L-BFGS training (requires closure function)\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            total_loss, params, physics_loss, data_loss, rhs, dCdt, arrhenius_term, I_mag = combined_loss(\n",
    "                t, T_C, T_C_norm, I, bat_idx, C_target, lambda_phys, lambda_data, epoch\n",
    "            )\n",
    "            total_loss.backward()\n",
    "            return total_loss\n",
    "        \n",
    "        total_loss = optimizer.step(closure)\n",
    "        \n",
    "        # Get current losses for logging\n",
    "        with torch.no_grad():\n",
    "            t_eval, T_C_eval, T_C_norm_eval, I_eval, bat_idx_eval, C_target_eval = prepare_batch(train_df)\n",
    "            t_eval.requires_grad_(True)  # Re-enable gradients for physics computation\n",
    "\n",
    "        total_loss, params, physics_loss, data_loss, rhs, dCdt, arrhenius_term, I_mag = combined_loss(\n",
    "            t_eval, T_C_eval, T_C_norm_eval, I_eval, bat_idx_eval, C_target_eval, lambda_phys, lambda_data, epoch\n",
    "        )\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Total Loss = {total_loss:.6f}, \"\n",
    "              f\"Physics Loss = {physics_loss:.6f}, Data Loss = {data_loss:.6f}, \"\n",
    "              f\"RHS = {rhs.mean().item():.6f}, dCdt = {dCdt.mean().item():.6f}, \"\n",
    "              f\"Arrhenius Term = {arrhenius_term.mean().item():.6f}, I_mag = {I_mag.mean().item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52aa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.000098\n",
      "Test MAE: 0.007406\n",
      "Test R²: 0.997463\n",
      "Test NRMSE: 0.006327 (0.63%)\n",
      "Test MAPE: 0.46%\n",
      "Test Max Error: 0.031207\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "def evaluate_model_normalized(test_df):\n",
    "    with torch.no_grad():\n",
    "        t, T_C, T_C_norm, I, bat_idx, C_target = prepare_batch(test_df)\n",
    "        \n",
    "        # Get embeddings and predictions\n",
    "        emb = Emb(bat_idx)\n",
    "        C_pred = ANN(t, T_C_norm, I, emb)\n",
    "        \n",
    "        # Convert to numpy for easier computation\n",
    "        y_true = C_target.numpy().flatten()\n",
    "        y_pred = C_pred.numpy().flatten()\n",
    "        \n",
    "        # Compute metrics\n",
    "        mse = torch.nn.MSELoss()(C_pred, C_target).item()\n",
    "        mae = torch.nn.L1Loss()(C_pred, C_target).item()\n",
    "        \n",
    "        # R² Score (coefficient of determination) - best metric for regression\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # Normalized RMSE (NRMSE) - RMSE normalized by mean target value\n",
    "        rmse = np.sqrt(mse)\n",
    "        nrmse = rmse / np.mean(y_true)\n",
    "        \n",
    "        # Mean Absolute Percentage Error (MAPE) - percentage-based error\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        \n",
    "        # Max error - worst single prediction\n",
    "        max_error = np.max(np.abs(y_true - y_pred))\n",
    "        \n",
    "        return {\n",
    "            'MSE': mse,\n",
    "            'MAE': mae,\n",
    "            'R2': r2,                    \n",
    "            'NRMSE': nrmse,              \n",
    "            'MAPE': mape,                \n",
    "            'Max_Error': max_error,     \n",
    "            'predictions': y_pred,\n",
    "            'targets': y_true\n",
    "        }\n",
    "\n",
    "# Test the model with comprehensive metrics\n",
    "test_results = evaluate_model_normalized(test_data)\n",
    "print(f\"Test MSE: {test_results['MSE']:.6f}\")\n",
    "print(f\"Test MAE: {test_results['MAE']:.6f}\")\n",
    "print(f\"Test R²: {test_results['R2']:.6f}\")\n",
    "print(f\"Test NRMSE: {test_results['NRMSE']:.6f} ({test_results['NRMSE']*100:.2f}%)\")\n",
    "print(f\"Test MAPE: {test_results['MAPE']:.2f}%\")\n",
    "print(f\"Test Max Error: {test_results['Max_Error']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f136c6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# In a new cell at the end of Train.ipynb\n",
    "torch.save(ANN.state_dict(), 'ann_model.pth')\n",
    "torch.save(Emb.state_dict(), 'emb_model.pth')\n",
    "torch.save(Param.state_dict(), 'param_model.pth')\n",
    "\n",
    "print(\"Models saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
